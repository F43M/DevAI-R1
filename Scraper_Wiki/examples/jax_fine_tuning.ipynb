{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning com JAX/Flax"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Exemplo simples usando transformers e Flax."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install -q transformers datasets flax optax"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from datasets import load_dataset\nfrom transformers import FlaxAutoModelForSequenceClassification, AutoTokenizer\nimport optax\n\ntrain_ds = load_dataset('imdb', split='train[:1%]')\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntrain_ds = train_ds.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n\nmodel = FlaxAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\noptimizer = optax.adam(5e-5)\nstate = model.create_train_state(optimizer=optimizer)\nfor batch in train_ds.shuffle(seed=0).batch(8):\n    state, metrics = model.train_step(state, batch)"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
