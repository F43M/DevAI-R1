{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning com MLflow"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Este notebook demonstra como treinar um modelo simples enquanto registra parametros e m\u00e9tricas no MLflow."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers datasets mlflow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport mlflow\n\ndataset = load_dataset('imdb', split='train[:1%]')\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntokenized = dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "mlflow.set_tracking_uri('http://localhost:5000')\nwith mlflow.start_run():\n    mlflow.log_param('model', 'bert-base-uncased')\n    mlflow.log_param('dataset_size', len(tokenized))\n    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n    args = TrainingArguments('out', num_train_epochs=1, per_device_train_batch_size=8)\n    trainer = Trainer(model=model, args=args, train_dataset=tokenized.select(range(32)), tokenizer=tokenizer)\n    trainer.train()\n    metrics = trainer.evaluate()\n    mlflow.log_metrics({k: float(v) for k, v in metrics.items()})"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
