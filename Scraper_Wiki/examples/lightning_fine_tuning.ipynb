{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning com PyTorch Lightning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Demonstra treino r\u00e1pido usando Lightning."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install -q pytorch-lightning transformers datasets"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pytorch_lightning as pl\nimport torch\n\ntrain_ds = load_dataset('imdb', split='train[:1%]')\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndef collate(batch):\n    enc = tokenizer([x['text'] for x in batch], padding=True, truncation=True, return_tensors='pt')\n    labels = torch.tensor([x['label'] for x in batch])\n    return {**enc, 'labels': labels}\n\nloader = torch.utils.data.DataLoader(train_ds, batch_size=8, collate_fn=collate)\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nclass LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = model\n    def forward(self, **x):\n        return self.model(**x)\n    def training_step(self, batch, batch_idx):\n        out = self(**batch)\n        loss = out.loss\n        self.log('train_loss', loss)\n        return loss\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=5e-5)\n\ntrainer = pl.Trainer(max_epochs=1, logger=False, enable_checkpointing=False)\ntrainer.fit(LitModel(), loader)"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
